import torch
from torchtext.data.utils import get_tokenizer
from torch.utils.data import DataLoader, TensorDataset
from config import BATCH_SIZE, MAX_LEN, VOCAB_SIZE, DEVICE
import os

# -------------------------- 1. æœ¬åœ°æ•°æ®é›†è·¯å¾„ï¼ˆå¿…é¡»å’Œä½ çš„æ–‡ä»¶ä½ç½®ä¸€è‡´ï¼ï¼‰--------------------------
DATASET_DIR = "../dataset"
TRAIN_PATH = os.path.join(DATASET_DIR, "wiki.train.tokens")
VAL_PATH = os.path.join(DATASET_DIR, "wiki.valid.tokens")
TEST_PATH = os.path.join(DATASET_DIR, "wiki.test.tokens")

# éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
assert os.path.exists(TRAIN_PATH), f"âŒ æ‰¾ä¸åˆ°è®­ç»ƒé›†æ–‡ä»¶ï¼è·¯å¾„ï¼š{TRAIN_PATH}"
assert os.path.exists(VAL_PATH), f"âŒ æ‰¾ä¸åˆ°éªŒè¯é›†æ–‡ä»¶ï¼è·¯å¾„ï¼š{VAL_PATH}"
assert os.path.exists(TEST_PATH), f"âŒ æ‰¾ä¸åˆ°æµ‹è¯•é›†æ–‡ä»¶ï¼è·¯å¾„ï¼š{TEST_PATH}"

# -------------------------- 2. è¯»å–æœ¬åœ°æ–‡æœ¬æ–‡ä»¶ï¼ˆä¸è”ç½‘ï¼‰--------------------------
def read_text_file(file_path):
    texts = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:  # è¿‡æ»¤ç©ºè¡Œ
                texts.append(line)
    return texts

# åŠ è½½æœ¬åœ°æ–‡ä»¶åˆ°æ–‡æœ¬åˆ—è¡¨
train_iter = read_text_file(TRAIN_PATH)
val_iter = read_text_file(VAL_PATH)
test_iter = read_text_file(TEST_PATH)

# -------------------------- 3. æ‰‹åŠ¨æ„å»ºè¯æ±‡è¡¨ï¼ˆé€‚é…æ‰€æœ‰torchtextç‰ˆæœ¬ï¼‰--------------------------
tokenizer = get_tokenizer("basic_english")  # è‹±æ–‡åˆ†è¯å™¨

# æ­¥éª¤1ï¼šç»Ÿè®¡è®­ç»ƒé›†ä¸­æ‰€æœ‰è¯çš„é¢‘ç‡ï¼ˆè¿‡æ»¤ä½é¢‘è¯ï¼‰
word_freq = {}
for text in train_iter:
    tokens = tokenizer(text)
    for token in tokens:
        if token in word_freq:
            word_freq[token] += 1
        else:
            word_freq[token] = 1

# æ­¥éª¤2ï¼šæŒ‰é¢‘ç‡æ’åºï¼Œå–å‰ VOCAB_SIZE-2 ä¸ªè¯ï¼ˆé¢„ç•™ <pad> å’Œ <unk>ï¼‰
sorted_words = sorted(word_freq.keys(), key=lambda x: word_freq[x], reverse=True)
selected_words = sorted_words[:VOCAB_SIZE - 2]  # å‡2æ˜¯å› ä¸ºè¦åŠ 2ä¸ªç‰¹æ®Šæ ‡è®°

# æ­¥éª¤3ï¼šæ„å»ºè¯æ±‡è¡¨ï¼ˆç‰¹æ®Šæ ‡è®°åœ¨å‰ï¼Œæ™®é€šè¯åœ¨åï¼‰
vocab_list = ["<pad>", "<unk>"] + selected_words
vocab = {word: idx for idx, word in enumerate(vocab_list)}  # å­—å…¸æ ¼å¼ï¼šè¯â†’ç´¢å¼•

# æ­¥éª¤4ï¼šå®šä¹‰è·å–è¯ç´¢å¼•çš„å‡½æ•°ï¼ˆæ›¿ä»£ vocab() æ–¹æ³•ï¼‰
def get_token_index(token):
    return vocab.get(token, vocab["<unk>"])  # æœªçŸ¥è¯è¿”å› <unk> çš„ç´¢å¼•ï¼ˆ1ï¼‰

# -------------------------- 4. æ–‡æœ¬è½¬å¼ é‡ï¼ˆå›ºå®šé•¿åº¦ï¼‰--------------------------
def text_to_tensor(text):
    tokens = tokenizer(text)
    # æˆªæ–­è¿‡é•¿åºåˆ—
    if len(tokens) > MAX_LEN:
        tokens = tokens[:MAX_LEN]
    # å¡«å……è¿‡çŸ­åºåˆ—
    else:
        tokens = tokens + ["<pad>"] * (MAX_LEN - len(tokens))
    # ç”¨æ‰‹åŠ¨å®šä¹‰çš„ get_token_index è½¬æ¢ç´¢å¼•
    token_indices = [get_token_index(token) for token in tokens]
    return torch.tensor(token_indices, dtype=torch.long, device=DEVICE)

# -------------------------- 5. æ„å»ºè¯­è¨€æ¨¡å‹æ•°æ®é›† --------------------------
def build_lm_dataset(data_iter):
    inputs = []
    labels = []
    for text in data_iter:
        tensor = text_to_tensor(text)
        if len(tensor) < 2:
            continue
        inputs.append(tensor[:-1])  # è¾“å…¥ï¼šå‰MAX_LEN-1ä¸ªè¯
        labels.append(tensor[1:])   # æ ‡ç­¾ï¼šåMAX_LEN-1ä¸ªè¯
    return TensorDataset(torch.stack(inputs), torch.stack(labels))

# ç”Ÿæˆæ•°æ®é›†
train_dataset = build_lm_dataset(train_iter)
val_dataset = build_lm_dataset(val_iter)
test_dataset = build_lm_dataset(test_iter)

# -------------------------- 6. æ„å»ºDataLoader --------------------------
train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    drop_last=True
)
val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    drop_last=False
)
test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    drop_last=False
)

# -------------------------- 7. æ‰“å°åŠ è½½æˆåŠŸä¿¡æ¯ --------------------------
print(f"âœ… æœ¬åœ°æ•°æ®é›†åŠ è½½æˆåŠŸï¼")
print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡ï¼š")
print(f"  - è®­ç»ƒé›†ï¼š{len(train_dataset)} æ ·æœ¬ | {len(train_loader)} æ‰¹æ¬¡")
print(f"  - éªŒè¯é›†ï¼š{len(val_dataset)} æ ·æœ¬ | {len(val_loader)} æ‰¹æ¬¡")
print(f"  - æµ‹è¯•é›†ï¼š{len(test_dataset)} æ ·æœ¬ | {len(test_loader)} æ‰¹æ¬¡")
print(f"  - è¯æ±‡è¡¨å¤§å°ï¼š{len(vocab)}")